{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Transformer for ARQMath \n",
    "\n",
    "NOTE: If you use Google Colab you need to enable GPU/TPU support first!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments)\n",
    "from transformers.data.data_collator import DataCollator\n",
    "from transformers.trainer import Trainer, set_seed\n",
    "from transformers.modeling_auto import AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers.tokenization_auto import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset import ARQMathDataset\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "seed = 0\n",
    "set_seed(seed)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul  2 19:02:16 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  On   | 00000000:04:00.0 Off |                  N/A |\n",
      "| 23%   24C    P8     7W / 250W |   3679MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  On   | 00000000:06:00.0 Off |                  N/A |\n",
      "| 28%   40C    P2    55W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  On   | 00000000:07:00.0 Off |                  N/A |\n",
      "| 22%   47C    P2    54W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  On   | 00000000:08:00.0 Off |                  N/A |\n",
      "| 22%   43C    P2    55W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 108...  On   | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 29%   31C    P2    52W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 108...  On   | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 23%   23C    P8     7W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 108...  On   | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 23%   21C    P8     7W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 108...  On   | 00000000:0F:00.0 Off |                  N/A |\n",
      "| 22%   24C    P8     9W / 250W |      1MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     17351      C   python                                      3667MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 19:02:16 - INFO - __main__ -   Using model from cache\n"
     ]
    }
   ],
   "source": [
    "cuda_visible_devices = '1,2,3,4' # check with nvidia-smi\n",
    "model_name = 'bert-base-cased'\n",
    "model_cache_dir = '/home/mostendorff/datasets/BERT_pre_trained_models/pytorch/'\n",
    "\n",
    "if os.path.exists(model_cache_dir + model_name):\n",
    "    logger.info('Using model from cache')\n",
    "    model_name_or_path = model_cache_dir + model_name\n",
    "else:\n",
    "    model_name_or_path = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = cuda_visible_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qID</th>\n",
       "      <th>aID</th>\n",
       "      <th>q</th>\n",
       "      <th>a</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "      <td>115</td>\n",
       "      <td>What are gradients and how would I use them?\\n...</td>\n",
       "      <td>The ∇ (pronounced \"del\") is an operator, more ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>125</td>\n",
       "      <td>How would you describe calculus in simple term...</td>\n",
       "      <td>There came a time in mathematics when people e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>148</td>\n",
       "      <td>How would you describe calculus in simple term...</td>\n",
       "      <td>One of the greatest achievements of human civi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118</td>\n",
       "      <td>517</td>\n",
       "      <td>How would you describe calculus in simple term...</td>\n",
       "      <td>Calculus is basically a way of calculating rat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>How would you describe calculus in simple term...</td>\n",
       "      <td>Calculus is a field which deals with two seemi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qID  aID                                                  q  \\\n",
       "0  112  115  What are gradients and how would I use them?\\n...   \n",
       "1  118  125  How would you describe calculus in simple term...   \n",
       "2  118  148  How would you describe calculus in simple term...   \n",
       "3  118  517  How would you describe calculus in simple term...   \n",
       "4  118  127  How would you describe calculus in simple term...   \n",
       "\n",
       "                                                   a  rel  \n",
       "0  The ∇ (pronounced \"del\") is an operator, more ...    1  \n",
       "1  There came a time in mathematics when people e...    1  \n",
       "2  One of the greatest achievements of human civi...    0  \n",
       "3  Calculus is basically a way of calculating rat...    0  \n",
       "4  Calculus is a field which deals with two seemi...    0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV from disk\n",
    "df = pd.read_csv('./data/qa-pair.csv', index_col=False)\n",
    "df['rel'] = df['rel'].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 154,744\n"
     ]
    }
   ],
   "source": [
    "print(f'Samples: {len(df):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    103957\n",
       "1     50787\n",
       "Name: rel, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample for debugging\n",
    "#df = df.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 19:02:17 - INFO - transformers.configuration_utils -   loading configuration file /home/mostendorff/datasets/BERT_pre_trained_models/pytorch/bert-base-cased/config.json\n",
      "2020-07-02 19:02:17 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "2020-07-02 19:02:19 - INFO - transformers.configuration_utils -   loading configuration file /home/mostendorff/datasets/BERT_pre_trained_models/pytorch/bert-base-cased/config.json\n",
      "2020-07-02 19:02:19 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "2020-07-02 19:02:19 - INFO - transformers.tokenization_utils_base -   Model name '/home/mostendorff/datasets/BERT_pre_trained_models/pytorch/bert-base-cased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/home/mostendorff/datasets/BERT_pre_trained_models/pytorch/bert-base-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-07-02 19:02:19 - INFO - transformers.tokenization_utils_base -   Didn't find file /home/mostendorff/datasets/BERT_pre_trained_models/pytorch/bert-base-cased/added_tokens.json. We won't load it.\n",
      "2020-07-02 19:02:19 - INFO - transformers.tokenization_utils_base -   Didn't find file /home/mostendorff/datasets/BERT_pre_trained_models/pytorch/bert-base-cased/special_tokens_map.json. We won't load it.\n",
      "2020-07-02 19:02:19 - INFO - transformers.tokenization_utils_base -   Didn't find file /home/mostendorff/datasets/BERT_pre_trained_models/pytorch/bert-base-cased/tokenizer_config.json. We won't load it.\n",
      "2020-07-02 19:02:19 - INFO - transformers.tokenization_utils_base -   Didn't find file /home/mostendorff/datasets/BERT_pre_trained_models/pytorch/bert-base-cased/tokenizer.json. We won't load it.\n",
      "2020-07-02 19:02:19 - INFO - transformers.tokenization_utils_base -   loading file /home/mostendorff/datasets/BERT_pre_trained_models/pytorch/bert-base-cased/vocab.txt\n",
      "2020-07-02 19:02:19 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "2020-07-02 19:02:19 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "2020-07-02 19:02:19 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "2020-07-02 19:02:19 - INFO - transformers.tokenization_utils_base -   loading file None\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_name_or_path, num_labels=2) \n",
    "model = AutoModelForSequenceClassification.from_config(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_df = df.sample(frac=train_size, random_state=seed)\n",
    "test_df = df.drop(train_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ARQMathDataset(train_df, tokenizer, 512)\n",
    "test_ds = ARQMathDataset(test_df, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2495, 14867, 12377,  9455, 23043,  1891, 19068,  1937,  2463,\n",
       "          6777,  1116,   178,  1169,   112,   189,  1256,  2373,  1142,   117,\n",
       "           178,  1138,  1185,  1911,  1184,   178,  1821,  1217,  1455,   119,\n",
       "           178,  1198,  1444,  1199,  1494,  1107,  7877, 12924,  2875,  2118,\n",
       "          1142,  6477,  1104,   170,  2304,   119,  6699,  1195,  1132,  1549,\n",
       "          1210,  1827,   113,   193,   117,   194,   114,   113,   193,  1475,\n",
       "           117,   194,  1475,   114,   113,   193,  1477,   117,   194,  1477,\n",
       "           114,  1105,  1195,  1328,  1106,  1525,   170,  3527, 18311, 15792,\n",
       "          1161,   185,   113,   193,   114,  1115,  4488,  1194,  1292,  1210,\n",
       "          1827,   119,  1142,  1110,  1549,  1103,  2495, 14867, 12377,  9455,\n",
       "         23043,  1891, 19068,   185,   113,   193,   114,   134,   194,   168,\n",
       "           121,   165,   175, 19366,   196,   113,   193,   118,   193,   168,\n",
       "           122,   114,   113,   193,   118,   193,   168,   123,   114,   198,\n",
       "           196,   113,   193,   168,   121,   118,   193,   168,   122,   114,\n",
       "           113,   193,   168,   121,   118,   193,   168,   123,   114,   198,\n",
       "           194,   168,   122,   165,   175, 19366,   196,   113,   193,   118,\n",
       "           193,   168,   122,   114,   113,   193,   118,   193,   168,   123,\n",
       "           114,   198,   196,   113,   193,   168,   121,   118,   193,   168,\n",
       "           122,   114,   113,   193,   168,   121,   118,   193,   168,   123,\n",
       "           114,   198,   194,   168,   123,   165,   175, 19366,   196,   113,\n",
       "           193,   118,   193,   168,   122,   114,   113,   193,   118,   193,\n",
       "           168,   123,   114,   198,   196,   113,   193,   168,   121,   118,\n",
       "           193,   168,   122,   114,   113,   193,   168,   121,   118,   193,\n",
       "           168,   123,   114,   198,   170,   114, 12434,  1115,   185,   113,\n",
       "           193,   168,   178,   114,   134,   194,   168,   178,  1111,   178,\n",
       "           134,   121,   117,   122,   117,   123,   102,  1103,  2495, 14867,\n",
       "         12377,  9455, 23043,  1891,  7893,  1128,  1724,  1205,  1110,  1136,\n",
       "          1268,   119,  1115,  1336,  1129,  1103,  2674,  1104,  1240,  2645,\n",
       "           119,  1195,  1209,  5633,  1240,  7893,   117,  1105,  1294,  1103,\n",
       "           113,  1317,   114,  3238,  2607,   119,   185,   113,   193,   114,\n",
       "           134,   194,   168,   121,   165,   175, 19366,   196,   113,   193,\n",
       "           118,   193,   168,   122,   114,   113,   193,   118,   193,   168,\n",
       "           123,   114,   198,   196,   113,   193,   168,   121,   118,   193,\n",
       "           168,   122,   114,   113,   193,   168,   121,   118,   193,   168,\n",
       "           123,   114,   198,   116,   194,   168,   122,   165,   175, 19366,\n",
       "           196,   113,   193,   118,   193,   168,   121,   114,   113,   193,\n",
       "           118,   193,   168,   123,   114,   198,   196,   113,   193,   168,\n",
       "           122,   118,   193,   168,   121,   114,   113,   193,   168,   122,\n",
       "           118,   193,   168,   123,   114,   198,   116,   194,   168,   123,\n",
       "           165,   175, 19366,   196,   113,   193,   118,   193,   168,   121,\n",
       "           114,   113,   193,   118,   193,   168,   122,   114,   198,   196,\n",
       "           113,   193,   168,   123,   118,   193,   168,   121,   114,   113,\n",
       "           193,   168,   123,   118,   193,   168,   122,   114,   198,   119,\n",
       "          1103,  1148,  1645,  1128,  1132,  1455,  1106,  1202,  1110,  1106,\n",
       "          1437,  1115,  1142,  7893,   107,  1759,   119,   107,  1106,  1202,\n",
       "          1115,   117,  1128,  1444,  1106, 23073,  1115,   185,   113,   193,\n",
       "           168,   121,   114,   134,   194,   168,   121,   117,  1105,   185,\n",
       "           113,   193,   168,   122,   114,   134,   194,   168,   122,   117,\n",
       "          1105,   185,   113,   193,   168,   123,   114,   134,   194,   168,\n",
       "           123,   119,   113,  1115,   112,   188,  1184,  1103,   107,   178,\n",
       "           107,  4333,  1110,  1164,   117,  1128,  1444,  1106, 23073,  1210,\n",
       "          1614,   102]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor(0)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "    \n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 19:02:20 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "2020-07-02 19:02:22 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=10,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    #warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    evaluate_during_training=True,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=2500,\n",
    "    #eval_steps=4000,\n",
    "    save_steps=-1,\n",
    "    save_total_limit=0,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_ds,       \n",
    "    eval_dataset=test_ds, \n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 19:02:23 - INFO - transformers.trainer -   ***** Running training *****\n",
      "2020-07-02 19:02:23 - INFO - transformers.trainer -     Num examples = 123795\n",
      "2020-07-02 19:02:23 - INFO - transformers.trainer -     Num Epochs = 3\n",
      "2020-07-02 19:02:23 - INFO - transformers.trainer -     Instantaneous batch size per device = 10\n",
      "2020-07-02 19:02:23 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 40\n",
      "2020-07-02 19:02:23 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "2020-07-02 19:02:23 - INFO - transformers.trainer -     Total optimization steps = 9285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da97bdd32afa44e88580c35afc62b9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870e06dfa0bf44ae8fe10727d29492ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=3095.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostendorff/miniconda2/envs/arqmath/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "2020-07-02 19:37:12 - INFO - transformers.trainer -   {'loss': 0.6318841715335846, 'learning_rate': 1.4614970382337104e-05, 'epoch': 0.8077544426494345, 'step': 2500}\n",
      "2020-07-02 19:37:12 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "2020-07-02 19:37:12 - INFO - transformers.trainer -     Num examples = 30949\n",
      "2020-07-02 19:37:12 - INFO - transformers.trainer -     Batch size = 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485e5154f64644b4b10760abc80d2445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=484.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 20:19:54 - INFO - transformers.trainer -   {'eval_loss': 0.6167520474919603, 'eval_accuracy': 0.671685676435426, 'eval_f1': 0.049396575919169246, 'eval_precision': 0.5751633986928104, 'eval_recall': 0.025806451612903226, 'epoch': 1.615508885298869, 'step': 5000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5786391abf34ac1a03cfa24af0823a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=3095.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 20:54:51 - INFO - transformers.trainer -   {'loss': 0.6143435514211655, 'learning_rate': 3.844911147011309e-06, 'epoch': 2.4232633279483036, 'step': 7500}\n",
      "2020-07-02 20:54:51 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "2020-07-02 20:54:51 - INFO - transformers.trainer -     Num examples = 30949\n",
      "2020-07-02 20:54:51 - INFO - transformers.trainer -     Batch size = 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80ae35bc3e842afa62463a88ad2ab5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=484.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 20:58:46 - INFO - transformers.trainer -   {'eval_loss': 0.6144362179454693, 'eval_accuracy': 0.6735274160716017, 'eval_f1': 0.18673535093367677, 'eval_precision': 0.528714676390155, 'eval_recall': 0.11339198435972629, 'epoch': 2.4232633279483036, 'step': 7500}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 21:23:45 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9285, training_loss=0.6199444479498095)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 21:23:45 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "2020-07-02 21:23:45 - INFO - transformers.trainer -     Num examples = 30949\n",
      "2020-07-02 21:23:45 - INFO - transformers.trainer -     Batch size = 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e11ebf6614e4743afa473b6c17d7641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=484.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 21:27:39 - INFO - transformers.trainer -   {'eval_loss': 0.6141976345299689, 'eval_accuracy': 0.6741736405053476, 'eval_f1': 0.18795297149299406, 'eval_precision': 0.533363802559415, 'eval_recall': 0.11407624633431085, 'epoch': 3.0, 'step': 9285}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 21:27:39 - INFO - __main__ -   {'eval_loss': 0.6141976345299689, 'eval_accuracy': 0.6741736405053476, 'eval_f1': 0.18795297149299406, 'eval_precision': 0.533363802559415, 'eval_recall': 0.11407624633431085, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "logger.info(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 21:27:39 - INFO - transformers.trainer -   ***** Running Prediction *****\n",
      "2020-07-02 21:27:39 - INFO - transformers.trainer -     Num examples = 30949\n",
      "2020-07-02 21:27:39 - INFO - transformers.trainer -     Batch size = 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900da5727dd343ff857afb6bcad5760d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Prediction', max=484.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_output = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qID</th>\n",
       "      <th>aID</th>\n",
       "      <th>q</th>\n",
       "      <th>a</th>\n",
       "      <th>rel</th>\n",
       "      <th>predicted_label_ids</th>\n",
       "      <th>predicted_0</th>\n",
       "      <th>predicted_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118</td>\n",
       "      <td>148</td>\n",
       "      <td>How would you describe calculus in simple term...</td>\n",
       "      <td>One of the greatest achievements of human civi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.805599</td>\n",
       "      <td>-0.734638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155</td>\n",
       "      <td>2329</td>\n",
       "      <td>How can you prove that a function has no close...</td>\n",
       "      <td>Have you ever heard of Galois theory? It is a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.730035</td>\n",
       "      <td>-0.681573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>255</td>\n",
       "      <td>548134</td>\n",
       "      <td>Why does the series \\sum_{n=1}^\\infty\\frac1n n...</td>\n",
       "      <td>Let's group the terms as follows:A=\\frac11+\\fr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.505219</td>\n",
       "      <td>-0.511177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>255</td>\n",
       "      <td>261</td>\n",
       "      <td>Why does the series \\sum_{n=1}^\\infty\\frac1n n...</td>\n",
       "      <td>This is not as good an answer as AgCl's, nonet...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.697238</td>\n",
       "      <td>-0.658140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255</td>\n",
       "      <td>1139726</td>\n",
       "      <td>Why does the series \\sum_{n=1}^\\infty\\frac1n n...</td>\n",
       "      <td>Let's assume that \\sum_{n=1}^{\\infty}\\frac1n=:...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.378869</td>\n",
       "      <td>-0.412837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qID      aID                                                  q  \\\n",
       "0  118      148  How would you describe calculus in simple term...   \n",
       "1  155     2329  How can you prove that a function has no close...   \n",
       "2  255   548134  Why does the series \\sum_{n=1}^\\infty\\frac1n n...   \n",
       "3  255      261  Why does the series \\sum_{n=1}^\\infty\\frac1n n...   \n",
       "4  255  1139726  Why does the series \\sum_{n=1}^\\infty\\frac1n n...   \n",
       "\n",
       "                                                   a  rel  \\\n",
       "0  One of the greatest achievements of human civi...    0   \n",
       "1  Have you ever heard of Galois theory? It is a ...    0   \n",
       "2  Let's group the terms as follows:A=\\frac11+\\fr...    0   \n",
       "3  This is not as good an answer as AgCl's, nonet...    0   \n",
       "4  Let's assume that \\sum_{n=1}^{\\infty}\\frac1n=:...    0   \n",
       "\n",
       "   predicted_label_ids  predicted_0  predicted_1  \n",
       "0                    0     0.805599    -0.734638  \n",
       "1                    0     0.730035    -0.681573  \n",
       "2                    0     0.505219    -0.511177  \n",
       "3                    0     0.697238    -0.658140  \n",
       "4                    0     0.378869    -0.412837  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = test_df\n",
    "pred_df['predicted_label_ids'] = prediction_output.predictions.argmax(-1)\n",
    "pred_df['predicted_0'] = prediction_output.predictions[:,0]\n",
    "pred_df['predicted_1'] = prediction_output.predictions[:,1]       \n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('./results/test_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20719\n",
       "1    10230\n",
       "Name: rel, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df['rel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    28761\n",
       "1     2188\n",
       "Name: predicted_label_ids, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df['predicted_label_ids'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.95      0.80     20719\n",
      "           1       0.53      0.11      0.19     10230\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     30949\n",
      "   macro avg       0.61      0.53      0.49     30949\n",
      "weighted avg       0.63      0.67      0.60     30949\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred_df['rel'].values, pred_df['predicted_label_ids'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 21:31:35 - INFO - transformers.trainer -   Saving model checkpoint to models/arqmath-bert-base-cased\n",
      "2020-07-02 21:31:35 - INFO - transformers.configuration_utils -   Configuration saved in models/arqmath-bert-base-cased/config.json\n",
      "2020-07-02 21:31:36 - INFO - transformers.modeling_utils -   Model weights saved in models/arqmath-bert-base-cased/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/arqmath-bert-base-cased/vocab.txt',\n",
       " 'models/arqmath-bert-base-cased/special_tokens_map.json',\n",
       " 'models/arqmath-bert-base-cased/added_tokens.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained model\n",
    "# model.save_pretrained(\"path/to/awesome-name-you-picked\")\n",
    "model_save_dir = 'models/arqmath-' + model_name\n",
    "\n",
    "os.makedirs(model_save_dir)\n",
    "trainer.save_model(model_save_dir)\n",
    "tokenizer.save_pretrained(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model to Huggingface after training\n",
    "\n",
    "See https://huggingface.co/welcome\n",
    "\n",
    "```bash\n",
    "transformers-cli login\n",
    "transformers-cli upload models/arqmath-bert-base-cased/config.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:arqmath] *",
   "language": "python",
   "name": "conda-env-arqmath-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
